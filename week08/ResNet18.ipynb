{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "num_classes = 10\n",
    "lr = 1e-3\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    custom_transform = {\n",
    "        \"train\": transforms.Compose([transforms.Resize([224, 224]),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.229, 0.224, 0.225])]),\n",
    "        \"test\": transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])])\n",
    "    }\n",
    "    train_ds = datasets.CIFAR10(root=\"./data\",\n",
    "                                train=True, transform=custom_transform[\"train\"],\n",
    "                                download=True)\n",
    "    test_ds = datasets.CIFAR10(root=\"./data\",\n",
    "                            train=False, transform=custom_transform[\"test\"], download=True)\n",
    "\n",
    "    val_size = 0.2  # 设置验证集的大小\n",
    "    num_train = len(train_ds)\n",
    "    indices = list(range(num_train))  # 获取所有的索引\n",
    "\n",
    "    np.random.shuffle(indices)  # 打乱索引\n",
    "\n",
    "    # 开始划分数据集\n",
    "    split = int(np.floor(val_size * num_train))\n",
    "    train_idx, val_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # 使用PyTorch的SubsetRandomSampler采样器采样\n",
    "    train_sampler, val_sampler = SubsetRandomSampler(\n",
    "        train_idx), SubsetRandomSampler(val_idx)\n",
    "\n",
    "    # 制作DataLoader\n",
    "    train_dl = DataLoader(\n",
    "        dataset=train_ds, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_dl = DataLoader(dataset=train_ds, batch_size=batch_size,\n",
    "                        sampler=val_idx)\n",
    "    test_dl = DataLoader(test_ds, batch_size)\n",
    "\n",
    "    return train_dl, val_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    基本残差块\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=[1,1], padding=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # 残差部分\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                      stride=stride[0], padding=padding),\n",
    "                      nn.BatchNorm2d(out_channels),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=stride[1], padding=padding),\n",
    "                      nn.BatchNorm2d(out_channels)  \n",
    "        )\n",
    "        #shortcut部分\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels,out_channels, kernel_size=1, stride=2, padding=0),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer(x) #残差部分\n",
    "        out += self.shortcut(x) #shortcut+残差部分\n",
    "        out = F.relu(out) #最后进行ReLU激活\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    \"\"\"\n",
    "    构建残差网络\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        # 第一层conv1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64,kernel_size=7, stride=2,padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2, 1)\n",
    "        )\n",
    "\n",
    "        # 叠加若干个BasicBlock\n",
    "        self.conv2 = self.make_layers(BasicBlock, [[1,1],[1,1]],64)\n",
    "        self.conv3 = self.make_layers(BasicBlock, [[2,1],[1,1]],128)\n",
    "        self.conv4 = self.make_layers(BasicBlock, [[2,1],[1,1]], 256)\n",
    "        self.conv5 = self.make_layers(BasicBlock, [[2,1],[1,1]], 512)\n",
    "\n",
    "        # 全连接层\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def make_layers(self, block, strides, out_channels):\n",
    "        \"\"\"\n",
    "        用于创建同一个残差块\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        # 每个conv模块都是2个BasicBlock组成\n",
    "        for stride in strides:\n",
    "            # 添加BasicBLock\n",
    "            layers.append(block(in_channels=self.in_channels, \n",
    "            out_channels=out_channels, stride=stride))\n",
    "            # 输出的维度变为下一个输入的维度\n",
    "            self.in_channels = out_channels\n",
    "\n",
    "        # 返回组成好的网络，*表示将列表打开\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18(num_classes)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置优化器和损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, val_dl):\n",
    "    for epoch in range(epochs):\n",
    "        val_acc, val_loss = 0., 0.\n",
    "        val_acc_list, val_loss_list = [], []\n",
    "        train_acc, train_loss = 0., 0.\n",
    "        train_acc_list, train_loss_list = [], []\n",
    "        total_num, total_correct = 0, 0\n",
    "        model.train() # 开始训练\n",
    "        for i, (features, targets) in enumerate(train_dl):\n",
    "            # 将数据放到GPU上\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播\n",
    "            logits = model(features)\n",
    "            # 计算损失\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            # softmax分类得到类别\n",
    "            probas = F.softmax(logits, dim=1)\n",
    "            # 获取分类的结果\n",
    "            preds = torch.max(probas, dim=1)[0]\n",
    "            # 计算正确的个数\n",
    "            total_correct += (targets == preds).sum().item()\n",
    "            # 计算总数\n",
    "            total_num += targets.size(0)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 打印输出信息\n",
    "            if i % 100 == 0:\n",
    "                print(\"epoch:{}/{}, iter:{}/{}, loss:{}\"\n",
    "                .format(epoch+1, epochs, i, len(train_dl), loss))\n",
    "        \n",
    "        # 添加损失值\n",
    "        train_loss_list.append(train_loss)\n",
    "        # 添加准确率\n",
    "        train_acc = float(total_correct) / total_num\n",
    "        train_acc_list.append(train_acc)\n",
    "        # 输出准确率以及损失值\n",
    "        print(\"train_acc{: .2f}, train_loss: {: .2f}\".format(train_acc, train_loss))\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0.  # 总损失\n",
    "            total_num = 0  # 总数\n",
    "            total_correct = 0  # 预测正确的个数\n",
    "\n",
    "            for features, targets in val_dl:\n",
    "                features = features.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                logits = model(features)\n",
    "                # 使用softmax得到分类的类别概率\n",
    "                probas = F.softmax(logits, dim=1)\n",
    "                preds = torch.max(probas, dim=1)[0]\n",
    "\n",
    "                total_correct += (preds == targets).sum().item()\n",
    "                total_num += targets.size(0)\n",
    "\n",
    "                loss = F.cross_entropy(targets, logits)\n",
    "                total_loss += loss.item()\n",
    "            # 添加到列表中\n",
    "            val_acc = float(total_correct) / total_num\n",
    "            val_acc_list.append(val_acc)\n",
    "            val_loss_list.append(val_loss)\n",
    "            print(\"val_acc{:.2f}, val_loss:{:.2f}\".format(val_acc, val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([3,2,1])\n",
    "print(type((a == b).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成随机数测试\n",
    "temp_num = torch.rand([32, 3, 32, 32]) #batch_size, channels, height, width\n",
    "temp_num = temp_num.to(device)\n",
    "temp_num.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.forward(temp_num)\n",
    "probas = F.softmax(out, dim=1) # 使用softmax得到各类别的概率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_class = torch.max(probas,dim=1)\n",
    "max_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用编写的方法进行测试\n",
    "train_dl, val_dl, test_dl = load_data()\n",
    "# train(model=model, train_dl=train_dl, val_dl=val_dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练好的模型和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = torchvision.models.resnet18(pretrained=True) # 使用定义好的网络，使用预训练模型\n",
    "# # 修改网络架构\n",
    "# net.fc = nn.Linear(512, num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ebf9cfd872009544a161647ac82c48f4cc096aba58631b69e515c7576d66293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
