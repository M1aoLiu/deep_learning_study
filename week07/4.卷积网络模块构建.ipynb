{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "input_size = 28 #MNIST数据集28*28\n",
    "num_classes = 10 #标签类别数\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "lr=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#为使用VGG16，将28*28->64*64\n",
    "custom_transform = transforms.Compose([transforms.Resize([64,64]),\n",
    "                                      transforms.ToTensor()])\n",
    "#训练集\n",
    "train_ds = datasets.MNIST(root=\"./\",\n",
    "                          train=True,\n",
    "                         transform = custom_transform,\n",
    "                         download=True)\n",
    "\n",
    "#测试集\n",
    "test_ds = datasets.MNIST(root=\"./\",\n",
    "                          train=False,\n",
    "                         transform = custom_transform)\n",
    "\n",
    "#使用dataloader加载数据\n",
    "train_dl = DataLoader(dataset=train_ds,\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True)\n",
    "test_dl = DataLoader(dataset=test_ds,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积网络模块构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2)\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2)\n",
    "        )\n",
    "        \n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128,\n",
    "                      out_channels=256,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256,\n",
    "                      out_channels=256,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256,\n",
    "                      out_channels=256,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2)\n",
    "        )\n",
    "    \n",
    "        self.block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256,\n",
    "                      out_channels=512,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512,\n",
    "                      out_channels=512,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512,\n",
    "                      out_channels=512,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2)\n",
    "        )\n",
    "        \n",
    "        self.block_5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,\n",
    "                      out_channels=512,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512,\n",
    "                      out_channels=512,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=512,\n",
    "                      out_channels=512,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.block_5(x)\n",
    "        logits = self.fc(x.view(-1, 512 * 2 * 2))\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        return logits, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(10)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "#定义设备\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc_and_loss(model, data_loader, device):\n",
    "    correct_pred, nums = 0, 0\n",
    "    total_loss = 0.\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "        # 将数据放到GPU上\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        #前向传播得到预测值以及softmax分类结果值\n",
    "        logits, probas= model(features)\n",
    "        \n",
    "        #计算总损失\n",
    "        total_loss += F.cross_entropy(logits, targets).item()\n",
    "        \n",
    "        #计算最大的结果标签\n",
    "        _, pred_labels = torch.max(probas, 1)\n",
    "        correct_pred += (pred_labels == targets).sum()\n",
    "        \n",
    "        #累加总数\n",
    "        nums += targets.size(0)\n",
    "    return float(correct_pred) / nums, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    #模型训练\n",
    "    model.train()\n",
    "    for i, (features, targets) in enumerate(train_dl):\n",
    "        # 将数据放到GPU上\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)        \n",
    "        #梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #前向传播\n",
    "        logits, prob = model.forward(features)\n",
    "        \n",
    "        #计算损失\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "                \n",
    "        #反向传播\n",
    "        loss.backward()\n",
    "                        \n",
    "        #参数更新\n",
    "        optimizer.step()\n",
    "        \n",
    "        #打印信息\n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch:{}/{}, iter:{}/{}, loss:{}\"\n",
    "                  .format(epoch+1,epochs,i,len(train_dl),\n",
    "                          loss))\n",
    "    \n",
    "    train_acc_list, train_loss_list = [], []\n",
    "    test_acc_list, test_loss_list = [], []\n",
    "    \n",
    "    #模型验证\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        train_acc, train_loss = compute_acc_and_loss(model, train_dl, device)\n",
    "        test_acc, test_loss = compute_acc_and_loss(model, test_dl, device)\n",
    "        train_acc_list.append(train_acc)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_acc_list.append(test_acc)\n",
    "        test_loss_list.append(test_loss)\n",
    "        print(\"train acc:{:.4f}, test acc:{:.4f}, test loss:{:.4f}\"\n",
    "              .format(train_acc, train_loss, test_acc, test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
